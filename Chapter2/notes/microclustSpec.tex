%---------------- COMMENT FOR IMPORTING ----------------------
%\RequirePackage{lineno}					%Comment for importing
\documentclass[a4document]{article}		%Comment for importing
%\pagestyle{headings}
%\input{ST-input2}								%Comment for importing
\input{../packages_input}
%\usepackage[authoryear,round]{natbib}
%\setcounter{chapter}{2}
\begin{document}								%Comment for importing
%\setpagewiselinenumbers
%\linenumbers
%\tableofcontents
%-------------------------------------------------------------

\chapter{Spectral Clustering for DataStreams}

\section{Introduction}

\section{Literature Review Notes}
\label{sec:lit-Clust}





\section{Spectral Clustering}
\label{sec:spec}
Data which is compact may be simple to cluster as the gaps between clusters are easier for simple clustering algorithms to identify. Sometimes data may be connected and therefore should hopefully be classed as a whole cluster, but it may not be tightly compact and spherical. Spectral clustering can provide good quality segmentation on even these difficult cases, however its performance comes at the cost of computational complexity. We shall use the Jordan-Weiss (NJW) spectral clustering framework which is described in Algorithm 1. 

\begin{algorithm}
\caption{NJW spectral clustering algorithm}
\begin{algorithmic}
\STATE Input: Data set $S = {x_1,\ldots, x_n  }$ and the number of clusters $k$
\STATE Output: $k$-way partition of the input data
\STATE Construct the affinity matrix $A$ by the following Gaussian kernel function:
\begin{equation}
  a_{i,j} = \exp  \left( - \frac{\| x_i - x_j \|^2}{2 \sigma^2} \right), i,j = 1, \ldots,n.
\end{equation}

\STATE Compute the normalized affinity matrix $L = D^{-\frac{1}{2}}A D^{-\frac{1}{2}}$, where $D$ is the diagonal matrix with $D_{ii}=\sum_{j=1}^{n}=a_{ij}.$
\STATE Compute the $k$ eigenvectors of $L$, $v_1, v_2,\ldots , v_k,$ associated with the $k$ largest eigenvalues, and form the matrix $ X = [v_1,v_2, \ldots , v_k]$.
\STATE Renormalize each row to form a new matrix $Y$.
\STATE Partition the n rows into k clusters via a general cluster algorithm, such as the K-means algorithm.
\STATE Assign the original point $x_i$ to the cluster $C$ $\iff$ the corresponding row $i$ of the matrix $Y$ is assigned to the cluster $C$.

\end{algorithmic}
\end{algorithm}

The affinity matrix $A$  is a metric that determines how close two points are in our space. A popular choice is to use $A$ to be the Gaussian kernel as in Algorithm 1. If $x_i$ and $x_j$ are very close, then $a_{i,j} \rightarrow 1 $, and if they are far apart $a_{i,j} \rightarrow 0$. If the user prefers a sparse affinity matrix instead of a fully connected graph, a cut off can be used by setting $a_{i,j} = 0$ when the distance between $x_i$ and $x_j$ is greater than some threshold. 

Spectral clustering can be challenging for very large data sets, constructing the affinity matrix $A$ and computing the eigenvectors of $L$ have computational complexity $\mathcal{O}(n^2)$ and $\mathcal{O}(n^3)$ respectively. REF proposes a fast approximate spectral clustering which uses a k-means preprocessing step to lessen the computational complexity whilst retaining good clustering performance. Firstly k-means is run on the whole dataset where $k$ is chosen to be large but such that $k \ll n$. The centers of the clusters are then used as representative datapoints for the whole dataset. Spectral clustering is performed on the representative set only, which is much easier than performing spectral clustering on the full dataset. The resulting cluster labels for the representative data are linked back to the original dataset such that every original datapoint acquire the same label as its associated $k$-means cluster center. This may not be the most sensible way to relate back the labelling information as discussed in REF. 

\begin{itemize}
\item Graph cut analogy
\item Discussion of self-tuning (Maybe later section)
\item Local interpolation
\item Talk through the algorithm more
\item Number of elements in the kmeans representative clusters
\end{itemize}

%Use the second eigenvector of the graph Laplcian is used to define a semi-optical cut.
% The Degree matrix is a diagonal matrix that measures the degree at each node $D_{i,i}=\sum^{n}_{j}a_{i,j} $.We use the Normalized Laplacian$ L_{N}=D^{-1/2}LD^{-1/2}$. Ideally, the Laplacian $L$ will be block-diagonal, where each block refers to a particular cluster.
\section{Datastream clustering}

A relatively new challenge to clustering is working with datastreams. Datastream clustering is clustering of data which arrives in an ordered sequence, continuously; for example, sensor data or shopping transactions. Much work has been done developing offline clustering methods, such as spectral clustering, but is it not suitable to apply these offline methods to the streaming scenario. Simply running an offline clustering algorithm on all the data observed so far may not be feasible. Often, the data cannot be stored in its entirety, therefore the older data has to be thrown away to make room for the new arrivals. This might at first seem like not a big issue as this will naturally weight the data temporally. However we may still wish to use older data in current clusterings. Also clustering algorithms can be computationally expensive. For example, in spectral clustering has $\mathcal{O}(n^3)$ when solving the eigenvalue problem. Therefore we need to be able to update our idea of the data as new points arrive efficiently and simply with little computational issues. 

Clustream (REF) offers a framework which allows quick and easy updates and the ability to perform sophisticated clustering algorithms. The big idea is to separate the clustering process in two stages;  a micro clustering which continuously updates statistical summaries of the dataset, and a macro clusterer which is run periodically. The microclusters are defined as a temporal extension of the cluster feature vector REF. 

Each micro-cluster is updated by storing the $d-dimensional$ vectors $CF1_x$, $CF2_x$ and value $n$, where $CF1_x$ is the sum of all observed data in that microcluster, $CF2_x$ is the sum of the squares of the data and $n$ is the number of elements in the micro-cluster. As these updates only require addition, they are cost effective but still informative. Critically it is possible to use these summaries to calculate the center of each micro-cluster.The time-stamps of the data can also stored in a similar additive manner which allows the user to isolate a particular window in the datastream and perform macro-clustering only on that instance. 

As new points in the datastream arrive, they are either allocated to a micro-cluster and the update procedure discussed above it carried out, or a new micro-cluster is created. At the point in which a macro clustering is acquired, spectral clustering is performed on the micro-cluster centers.

The way that micro-clusters are updated can effect the macro clustering output. It is vital to ensure that the micro-cluster well represent the underlying dataset or else the macro clustering will under perform. It is possible that highly dense regions may be represented in a similar way to less dense micro clusters. Note that when we pass information from the micro-clusters to the macro clusters, only centers of the micro-clusters are used, and there can be information loss. Cluster A may have 120 elements whist cluster B may only contain 10, however as new representative points in the macro clustering, there will be treated the same. Therefore it is important to try and increase the number of micro-clusters where there is more data, and limit them where data is sparse. Or somehow pass the number of elements in a micro-clustering into the macro-clustering algorithm. 

In order to achieve this we suggest putting a limit on the number of elements in a micro-cluster. We are already sorting this information in the micro-clustering updating system so this amendment would not change the storage requirements, however it does add an extra computational step. This means that the number of micro-clusters may have to vary as we progress through the stream which may prove challenging. Also this means that once a micro cluster is at maximum capacity. This means it will no longer be updated, and may be deleted after sufficient time has passed. 

\section{Micro-cluster based spectral clustering}
\label{sec:microSpec}

Our intention is to a develop method for applying spectral clustering to a datastream. In order to achieve this we adapt a micro-clustering type approach to quickly update a summary of the data. When an overall clustering is required, spectral clustering is performed using the centers of the micro-clusters as the input data. The micro-clusters act as a way of summarising the constantly arriving datasteam whilst allowing updates to occur in a non intrusive, non-computationally difficult manner, with limited storage requirements. 

We decided to put a limit on the radius of the micro-clusters. This is because we want the micro-clusters to well represent the underlying data in order to get good spectral clustering output. We also decided to limit the number of elements that belong to a cluster. However this may be difficult to work with theoretically as this means that the number of micro-clusters $k$ is not fixed. This may not turn out to be important as we are going to compute the true number of macro clusters $C$ in the spectral clustering macro clustering section. 

Our algorithm is as follows: 
\begin{itemize}
\item  Initialise micro-clusters
\item New datapoint arrives, update micro-clusters in the manner discussed. Either allocate the datapoint to an existing micro-cluster or initialise a new micro-cluster. 
\item At certain intervals take the centers of the micro-clusters as input to a spectral clustering algorithm. 
\item Then link the spectral clustering output back to the original datapoints (if required) either simply by assuming that each cluster member takes the same result as its cluster center, or a more complicated weighted method. 
\end{itemize}

We are (hopefully) able to provide guarantees on the performance of this algorithm, based directly on the way in which the data is represented. The nature of this algorithm allows the user to get close to online streaming and perform spectral clustering on a summary of the whole of the dataset. Infact by utilising the time stamping method in clustream, we should be able to perform spectral clustering on just certain windows of the data. 

\section{Performance}
\label{sec:perf}

In order to investigate the performance of our algorithm we will asses it on both simulated and real data(texture). Standard offline spectral clustering will be compared against the streaming spectral clustering. The affinity matrix will be used as described in Section \ref{sec:spec}. V-measure and purity will be used to interpret cluster performance, and specifically a misclustering rate will be used to compare directly against the offline case.  

REF introduces the misclustering rate to quantify the difference between an original and perturbed dataset described as 
\begin{equation}
  \label{eq:misClust}
  \rho = \frac{1}{n} \sum_{i = 1}^{n} \mathbb{I}(I_i \neq \tilde{I}_i)
\end{equation}

Here $\mathbb{I}$ is the indicator function, $I_i$ represents the cluster allocated to original datapoint $x_i$ and $\tilde{I_i}$ represents the cluster allocation of perturbed datapoint $\tilde{x}_i$. If $\rho = 1$ then the clustering of the perturbed data is identical to the clustering of the original data. In Section \ref{sec:bounds} we will attempt to justify bounds on the values of the misclustering $\rho$ given some understanding of the micro-clustering process used. 
 

%\begin{figure}[h]
%  \centering
 %  \caption{Graphs etc. here}
%\end{figure}

%To compute purity , each cluster is assigned to the class which is most frequent in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by $N$. Formally:
%\begin{equation}
%\mbox{purity}(\Omega,C) = \frac{1}{N} \sum_k \max_j \vert \omega_k \cap c_j\vert
%\end{equation} 	
%where $\Omega = \{ \omega_1, \omega_2, \ldots, \omega_K \}$is the set of clusters and $C = \{ c_1,c_2,\ldots,c_J \}$ is the set of classes Bad clusterings have purity values close to 0, a perfect clustering has a purity of 1 .Purity can become very high when there are a large number f clusters and therefore there it is not suitable for all cases, despite its nice simplicity. The V-Measure (A. Rosenberg and J. Hirschberg, 2007.) is described the harmonic mean of the homogenity and the completeness. This allows. Both these measures will allow us to compare the overall performance of the clustering given we know the ideal clustering scenario. However when working online with massive datasets we may be happy to just get a performance as good as or as close to the offline performance as possible

\section{Theoretical Guarantees and Bounds}
\label{sec:bounds}
Aim is t investigate the impact of clustering performance of perturbation on the data. The data perturbation is modelled as noisy observations. 
This section aims to establish a link between the parameters used in streaming k-means updates of the microclusters, and the finial effect this has on the overall clusteing of the dataset in terms of misclustering rate. 

First to summarise the work in REF KASP. Fast Approximate Spectral Clustering is an offline algorithm for static datasets. A k means algorithm was used to generate a representative dataset to apply on spectral clustering. This representative dataset was made up of the centroids found from running kmeans with k clusters. 

The stage of choosing a set of representative points that are most suited to well represent a data set is know as vector quanitzation in electrical engineering. Often to achieve this we wish to minimize a distortion measure, and is we use the squared error this naturally leads to a kmeans implementation. In KASP, k-means is used as a preprocessor for spectral clustering. 

We revisit the assumptions made in KASP for the perrtuabtion aanalysis (offline case). 

They model the perturbation of the data as an additive noise model. They assume that the original data $x_1, \ldots, x_n$ are i.i.d. according to a probability distribution $G$, and the data perturbation is represented by addind a noise component $\epsilon_i$ to $x_i$ as shown. 

$\tilde{x_i} = x_i + \epsilon_i$

Other assumptions are $\epsilon_i$ is independent of $x_i$, $\epsilon_i$ are i.i.d according to a symmetric distribtion with mean 0 and bounded support. Variance of $\epsilon_i$ is small relative to that of the original data. 

In orer to quantify and bound the perturbations on the Laplacian matrix, a statistical model is implmented. As the KASP authors work only with the bipartition case and then expand to the multi cut version. it is assumed that the data falls naturally ino two clusters. Therefore G is modelled as a two component mixture model

\begin{equation}
  G = (1 - \pi) \cdot G_1 + \pi \cdot G_2,
\end{equation}

where $\pi \in \left\{0,1\right\}$. When perturbed by the data, the model now becomes

\begin{equation}
  \tilde{G} = (1 - \pi) \cdot \tilde{G_1} + \pi \cdot \tilde{G_2,}.
\end{equation}
 

It is shown that the second eigenvector of the Laplacian matrix of the representative points (with repetitions) can be computed from the set of unique representative points. This cna provide a big speed up for significantly large $n$. 

Specific KASP analysis. 

The noise component models the difference between each original data  $x_i$ and it's associated microcluster center. 

KASP investigated the relationship between data perturbation and the misclutering rate $\rho$ and determined a link between the perturbation on the Laplacian matrices and $\rho$, which is reprinted here. 

Assume that: 1) the assumptions of Theorem 1(list) hold throughout the recursive invocation of the Ncut algorithm, 2) the smallest eigengap
$g_0$ along the recursion is bounded away from zero, and 3) the Frobenius norm of the perturbation on Laplacian matrices along the recursion is bounded by $c\|\tilde{L} - L\|_F^2$ for some constant $c \geq 1$. Then the mis-clustering rate for an m-way spectral clustering solution can be bounded by

\begin{equation}
  \label{eq:KASPThm3}
  \rho \leq \frac{m}{g_0^2} \cdot c \| \tilde{L} - L \|_F^2.
\end{equation}

This provides a link between the perturbation of the original dataset and the perturbation of the Laplacian matrix. 

\begin{itemize}
\item Build in more of existing theory
\item Discussion of required assumptions
\item Construct a model structure for the micro-clustering stage
\end{itemize}

\newpage 

\section{Sequential K-means}

Instead of updating the k-mean in bacthes, it is possible to update the means one at a time, sequentially.

Initally it is necessary to guess locations for the means. 
Initialise $n_1, n_2, ..., n_k = 0$
   Read in the new data point $x$
    If $\mu_i$ is closest to $x$
        $n_i = n_i+1$
        $\mu_i = \mu_i + (1/n_i)*(x - \mu_i) $  


If wish to incorporate a forgetting feature to encorporate drift, we can ignore the $n_i$ terms and just 


Blah

Streaming K-means approximatio Ailon/Jaiswal/Monteleoni 

One-pass streaming setting



%\section{Conclusion}


Issues with kmeans, can have exponential running time in worst case  and only provides a locally optimum solution. No guarentee it will be close to the global optimum. Despite the lack of guarntees fror kmeans,  in practise it is still popular as it is simple and often fast. Improving the initilasation procedure has been the focus on recent work. Different initialization preceedures can has a large effect on both the algorithm guarantees and quality of solution. 

K means ++ is an initialization algorithm from Arthur and Vassilvitski. ONly the first cluster center is chosen randomly, and then others are chosen with preference given to centers taht further away than the ones already defined which is sensible given the assumption that the data is well clusterable. 


%---------------- COMMENT FOR IMPORTING ----------------------
\pagebreak											%Comment for importing
\bibliographystyle{plainnat}		%Comment for importing
%\bibliography{References}				%Comment for importing
\bibliography{/home/rhian/Dropbox/library,/home/rhian/Dropbox/rhian}
\end{document}									%Comment for importing
%-------------------------------------------------------------



