%---------------- COMMENT FOR IMPORTING ----------------------
%\RequirePackage{lineno}				
%\documentclass[12pt]{report}		
%\pagestyle{headings}
%\input{ST-input2}							

%\setcounter{chapter}{0}
%\begin{document}								
%\setpagewiselinenumbers
%\linenumbers
%\tableofcontents
%-------------------------------------------------------------

\chapter{Introduction}

%Notes on writing - Pyramid system
% ideas at any level must be summaries of ideas gruoped below them
% Ideas in each grouping must be of the same kind of idea
% Ideas in each grouping must be logically ordered
% Situation, Complication, Question, Answer
% Mutually exclusive, collectively exhaustive.

In 2010, the giant American store Macy's was manually entering customer data into Excel.  Recently, they changed their strategy, collecting customer interactions automatically in-store, online and even using twitter streams to develop a dynamic pricing system. Now, every day they are capable of altering the prices of 73 million items in near real-time based on estimated customer demand and current inventory. Sales increased by 10\%.

Meanwhile, the energy sector has seen the creation of the digital oil field. By placing sensors directly onto drilling and pumping machines it is possible for energy companies to detect tiny leaks in piping almost immediately, enabling them to fix the problem before causing environmental damage and loss of product. These sensors can also be used to predict where the next leak is likely to occur, allowing  preventative repairs to be made.

In 2012, Kent police travelled to Los Angeles to learn about PredPol from the L.A.P.D. This software uses the date, time and location of logged crimes to predict potential crime hot spots allowing the police to target patrols to those areas. A recent study \citep{Mohler2015} suggests that using this system lead to 1.4-2.2 times as much crime detected compared to using a dedicated crime analyst. % leading to an average 7.4\% reduction in crime volume.

%In 2001 analyst
What do these three stories have in common? The use of big data to understand systems and improve performance. Big data \citep{Buhlmann2016} is a term used to refer to data sets so huge that traditional statistical techniques may not be directly applicable. But what is it exactly that makes big data big and what are the research challenges and opportunities arising from big data? Doug Laney defined big data in terms of the three V's; volume (amount of data), variety (range of data types and sources) and velocity (speed of incoming data). We will briefly consider the three V's in turn finishing with velocity which is central to this thesis. 

The volume of data collected on a daily basis is staggering.  In 2013,  IBM stated  that over 90\% of the world's data was created in the last two years. This creates a challenge for researchers and practitioners as computers may not have the memory requirements to deal with such large quantities of data, and their algorithms may not run fast enough or even at all. This has lead to the rise of new statistical algorithms specifically designed to deal with big data and an increase in research in areas such as parallel computing and sparse matrix representation. However, this aspect of big data may have been over hyped. Both computational power and digital storage costs are cheap and becoming cheaper all the time, allowing us to analyse massive data sets in a suitable time frame. Even if we cannot analyse the full data set, it is possible to \textit{sample} the data, taking a representative sample of the population, and extrapolating the results to the full data set. In fact, if we wish to infer something about a small group of specific customers, having a large volume of data to start with can actually be a statistical advantage as it is likely that there will be a large enough sample size to perform analysis. So yes big data is voluminous, but given the availability of cheap, fast computing, this might not be as challenging as it first seems. 

Variety relates to the many different types of data it is possible to collect  mainly due to the Internet and the ability to place sensors almost anywhere. Smart phones are constantly collecting and transmitting data to a variety of apps such as your location, the orientation of your phone and the photo you have just taken of your lunch. Wonga.com, a popular payday loan company claims to use over 8,000 different features when deciding whether to grant a customer a loan, a decision which is made in less than six minutes. Instead of  using just the customer's financial history, Wonga can collect information on the browser you use to visit their website, an estimate of your location from your I.P address and the time of day that you access their website. They also encourage customers to link their Facebook profile to their Wonga account, allowing Wonga access to details such as your job, religious views, hobbies and even the news feed of your friends activities.  All of this information can be combined with your financial history to build a fuller picture to predict whether or not you are likely to pay back the loan. 

With such great variety of data features available a challenge arises - the curse of dimensionality. As the number of features increases it becomes difficult to use some techniques which work well in lower dimensions. Data points become increasingly sparse as the dimensionality increases \citep{Steinbach2004} and distances metrics such as the Euclidean distance which is required to calculate k-means clustering become meaningless. One research area to emerge from the challenge of high-dimensionality is dimension reduction. These techniques aim to reduce the effect of irrelevant features by finding a low dimensional representation of the data whilst retaining as much information as possible. Once transformed into a lower dimensional representation, analysis will be computationally feasible. One common dimension reduction technique is Principle Component Analysis (PCA). In PCA, the aim is to explain the maximum amount of variance in the data set using the fewest number of principle components. Although this is an interesting area of research we do not consider dimension reduction any further in this thesis. 

%Wonga can make this decision within 6 minutes of your request and the money can be in your account in just 15 minutes. nearly 8,000 data features. Summarise this paragraph. (Variety not as challenging as velocity). they have a 7\% default rate, lower than the standard for UK banks. the ability to perform \textit{sentiment analysis}

Velocity is the aspect of big data which can be the most challenging, and is central to this thesis. Data which arrives in an ordered sequence, continuously can be considered a \textit{data stream} \citep{Aggarwal2007, Gama2007}. The velocity relates to the rate at which the data arrives, perhaps millions of data points each hour. Examples of data streams include telecommunications, meteorological data, the stock market, sensor networks,  online shopping and social media. Many statistical techniques assume a finite amount of data, generated by some unknown stationary probability distribution which can be physically stored and analysed in batch. However, there are restrictions imposed by data streams which makes using standard techniques not possible \citep{Silva2013}:

\begin{enumerate}
\item Data objects arrive continuously.
\item There is no control over the order in which the data objects should be processed.
\item The size of the stream is potentially unbounded.
\item Data objects are discarded after they have been processed.
\item The unknown data generation process is possibly non stationary.
\end{enumerate}

Given the restrictions above, how can we perform analysis such as finding patterns or clusters on a non-stationary data stream? Sampling a data stream isn't as simple as sampling purely static data due to restrictions 2 and 4. We could sample by selecting a window of the most recent data however this will infer a temporal bias and we may miss important seasonal trends such as the effect of Christmas on shopping sales. We require a \textit{representative data summary} to be calculated in such a way that analysis can still be performed.  New methods are required to create useful data summaries and to use these summaries in a sensible way to perform analysis on the stream. 

Sometimes, even transmitting data from it's point of collection to a research centre can be challenging either because the size of the data is so big, or because it is difficult to broadcast the data stream over a network. Often scientists are interested in exploring remote areas, placing sensors to track animals in deep jungle, monitoring ice levels in the Arctic or sending images taken on Mars back to Earth. The issue of transmitting data and the key role that data summaries play is detailed in the next example.

Insects borne disease like malaria kill at least one million people a year \citep{Murray2012}. There are 3,528 different species of mosquito, but less than 5\% of them carry human disease and only the females suck blood. If we could accurately classify mosquitoes in the field in real time, we could implement cheap and effective localised intervention campaigns. Professor Eamonn Keogh from the University of California Riverside \citep{Chen2014, Harkness2016} has created a way to sort insects, classifying them by species and gender based on the pitch of their whine. His goal is to produce cheap sensors which can be dropped and left in remote locations. As mosquitoes fly past the sensors, an audio file of their whine is recorded, along with a time stamp and other environmental factors. In this setting each  mosquito which flies past the sensor is a data point in the data stream. Transmitting the raw audio files to a central server as they are collected would be very expensive especially as we are in a remote location. Instead, a data summary is calculated by the sensor at location and the summary is transmitted to a central server at specific intervals. At the central server, the data summaries from this sensor and many other sensors can be collated and used for a more intensive detailed analysis.

This illustrates the central theme of this thesis. Transforming a data stream into an informative yet efficient data summary, and unravelling that summary to infer about the data stream as a whole. This raises a number of challenges. The data summary must be efficient to store and transmit and quick to update. Once transmitted to a central server we must be able to still perform analysis on the summaries and ideally have some idea of how good this is as compared to if we could perform analysis on the full data stream. 

%- How can we summarise a stream?  Simplest technique is windowing. This is easy but creates a bias to local data. Could be a problem when trying to deal with long term trends. The most popular method is Clustream. 
%- How can we perform analysis on summarised stream data, and how does it differ to running on the full data set?

%Finding patterns and regularities in big data sets (financial, scientific, social media, medical, online shopping)
%Transmission costs can also be high, especially when collecting from remote locations. The need to constantly collect and send data from remote locations (whether a plain in central Africa, or a crater on Mars)


%\section{Clustering}
%What is a cluster. Why do we care?
%Finding patterns in data. /Big data. Yes there is a hype. Space is cheap.
%streaming data
%requires new methods
%Astronomy. Space example, tracking comets/asteroids, images from Mars
%Financial/shopping data large sample. Bias - recent data won't capture Christmas shopping trends. When we throw away data what do we lose?
%Clustering images. Define clustering

\section{Thesis Outline}
\label{sec:outline}
%\ref{chap:spectral}
The thesis consists of three chapters each of which is self contained and covers a separate research area. Chapter 2 concerns the clustering of data streams, Chapter 3 is a data-driven case study and Chapter 4 considers the problem of detecting foreground in videos which have been compressed. Chapter 4 is a published conference paper \citep{Davies2013} and was awarded Best Paper at the Sensor Data Fusion Workshop 2013. A summary of each of the forthcoming chapters is outlined below. 

In Chapter 2 we consider the problem of identifying groups or clusters in a data stream.  Many different types of clustering algorithms exist, centroid type methods such as k-means \citep{MacQueen1967, Lloyd1982} and density based algorithms like DB-Scan \citep{Ester1996}. We restrict our interests to \textit{Spectral Clustering}, a clustering method which uses the eigenvalues of an \textit{affinity matrix} of the data to perform dimension reduction and cluster in the lower dimensional space. Spectral Clustering is popular, offers good empirical performance and can handle tricky, non standard data sets. However there does not exist an algorithm for performing Spectral Clustering on data streams.

A popular algorithm used to handle data streams is the Clustream algorithm \citep{Aggarwal2003}. We apply Clustream to summarise the data stream and perform Spectral Clustering on those data summaries only which has not been explored before in the literature. We investigate how best to incorporate the data summaries in to the Spectral Clustering algorithm by comparing both a weighted and non-weighted methods. A number of different data sets are considered including handwritten digit data and wavelet based texture features from an image data set.   

%\ref{chap:datasets}
Chapter 3 is an application driven study of a real data stream of online car insurance quotes gathered over a period of three months.  We analyse the data stream and investigate the potential loss of information that may arise from using data summaries on a data stream. \RD{Add more when Chapter 3 is completed}.

%\ref{chap:CSBGS}.
Chapter 4 is concerned with the identification of activity in surveillance videos, also called \textit{background subtraction}. Current methods for this require storing each pixel of every video frame which can be wasteful as most of this information refers to the uninteresting background. \textit{Compressive sensing}, a method for summarising data, can offer an efficient solution by utilising the fact that foreground is often sparse in the spatial domain. By making this assumption and applying a specific recovery algorithm to a trained background, it is possible to reconstruct the foreground, using only a low dimensional representation of the difference between the current frame and the estimated background scene. Chapter 4 provides a study of the effect of two different recovery algorithms on performance of background subtraction.

% \ref{chap:conc}

All three of the chapters are linked by the challenge of gaining insight from data streams by analysing a simple summary of the stream and extrapolating the analysis to learn something about the data stream as a whole. Chapter 2 specifically considers how Spectral Clustering performs on data summaries. Chapter 3 is a study on a real data stream \RD{TBC}. In Chapter 4, when performing background subtraction on a compressed video, we are again using a data summary (the compressed version of the video) to perform an analysis (segmenting the stream) crucially without transmitting the data as a whole (the full video stream) which would be much more computationally challenging. 

General thoughts on using data summaries to analyse data streams are discussed in Chapter 5 along with suggestions for future areas of research.

%---------------- COMMENT FOR IMPORTING ----------------------
%\pagebreak											%Comment for importing
%\bibliographystyle{plainnat}		%Comment for importing
%\bibliography{References}				%Comment for importing
%\end{document}									%Comment for importing
%-------------------------------------------------------------
